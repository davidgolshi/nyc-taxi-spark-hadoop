{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Project: Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTES:\n",
    "## TODO: dotenv configuration\n",
    "#  TODO: Commenting and Formatting\n",
    "\n",
    "## Alternative Methods: \n",
    "# [] url read into pandas and then export to parquet (pandas to pyarrow export? or pandas export?)\n",
    "# [] incorporate gpu or parallelization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.csv as csv\n",
    "from pyarrow import fs\n",
    "from hdfs import InsecureClient\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick script to create all year-month dates for nyc-taxi files\n",
    "dates = []\n",
    "for year in list(map(str, range(2017, 2021))):\n",
    "    for month in list(map(str, range(1, 13))):\n",
    "        if len(month) == 1:\n",
    "            month = '0' + month\n",
    "        else:\n",
    "            pass\n",
    "        date = year + '-' + month\n",
    "        dates.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over urls to download files\n",
    "# nyc-taxi files are stored on s3 with a standard filename convention\n",
    "failed_links = []\n",
    "csv_file_path = os.path.join('./data', 'staging', 'nyc-taxi')\n",
    "for i, date in enumerate(tqdm(dates)):\n",
    "    filename = f'yellow_taxi_{date}.csv'\n",
    "    if filename in os.listdir(csv_file_path):\n",
    "        print(f'File already exisits: {csv_file_path+filename}')\n",
    "        continue\n",
    "    else:\n",
    "        url = f\"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_{date}.csv\"\n",
    "        print(f'fetching yellow taxi {date} data from: \\n{url}')\n",
    "        try:    \n",
    "            file_req = requests.get(url)\n",
    "            url_content = file_req.content\n",
    "            csv_file = open(f'{csv_file_path}/yellow_taxi_{date}.csv', 'wb')\n",
    "            csv_file.write(url_content)\n",
    "            csv_file.close()\n",
    "        \n",
    "        except ConnectionError:\n",
    "            failed_links.append(url)\n",
    "\n",
    "print(f'{i+1 - len(failed_links)}/{i+1} files successfully fetched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total csv data size: 29.16 GB\n"
     ]
    }
   ],
   "source": [
    "# check total size of data files in csv format\n",
    "csv_size = round(sum(os.path.getsize(os.path.join(csv_file_path, f)) for f in os.listdir(csv_file_path))/1e9, 2)\n",
    "\n",
    "print(f'Total csv data size: {csv_size} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lookup table for taxi location ids\n",
    "url = 'https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv'\n",
    "print(f'fetching yellow taxi locations table...')\n",
    "file_req = requests.get(url)\n",
    "url_content = file_req.content\n",
    "csv_file = open(f'./data/taxi_location_lookup_table.csv', 'wb')\n",
    "csv_file.write(url_content)\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert csv files to parquet files\n",
    "parquet_file_path = os.path.join('./data', 'nyc-taxi')\n",
    "tq_csv_files = tqdm([file for file in os.listdir(csv_file_path)])\n",
    "for csv_file in tq_csv_files:\n",
    "    parquet_file = csv_file.replace('csv', 'parquet')\n",
    "    if parquet_file in os.listdir(parquet_file_path): \n",
    "        print(f'{parquet_file} already exists')\n",
    "        continue\n",
    "    tq_csv_files.set_description(f'Converting {csv_file} to parquet...')\n",
    "    try:\n",
    "        table = csv.read_csv(f'{csv_file_path}/{csv_file}')\n",
    "    except pa.lib.ArrowInvalid:\n",
    "        df = pd.read_csv(f'{csv_file_path}/{csv_file}', low_memory=False)\n",
    "        table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'{parquet_file_path}/{parquet_file}', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parquet data size: 5.64 GB\n"
     ]
    }
   ],
   "source": [
    "# check total file size of parquet data files\n",
    "prqt_size = round(sum(os.path.getsize(os.path.join(parquet_file_path, f)) for f in os.listdir(parquet_file_path)) / 1e9, 2)\n",
    "print(f'Total parquet data size: {prqt_size} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "HdfsError",
     "evalue": "Remote path '/user/hadoop/input/yellow_taxi_2019-07.parquet' already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_78115/4282104671.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparquet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlocal_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nyc-taxi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparquet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mhdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdfs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mhdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdfs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/anaconda3/envs/Spark/lib/python3.8/site-packages/hdfs/client.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(self, hdfs_path, local_path, n_threads, temp_dir, chunk_size, progress, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlocal_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mHdfsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Remote path %r already exists.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdfs_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mtemp_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdfs_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHdfsError\u001b[0m: Remote path '/user/hadoop/input/yellow_taxi_2019-07.parquet' already exists."
     ]
    }
   ],
   "source": [
    "# load data into hdfs\n",
    "namenode_URI = 'http://localhost:9870'\n",
    "hadoop_user = 'hadoop'\n",
    "\n",
    "hdfs = InsecureClient(namenode_URI, user=hadoop_user)\n",
    "hdfs_path = '/user/hadoop/input/'\n",
    "\n",
    "for parquet in os.listdir(parquet_file_path):\n",
    "    local_path = os.path.join('./data', 'nyc-taxi', parquet)\n",
    "    hdfs.upload(hdfs_path, local_path, n_threads=4)\n",
    "\n",
    "hdfs.list(hdfs_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "018058fd1a9831f8202021ce3d2ea0f1237b6cfb311c91aea6b7cd2f2c610067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('Spark': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
