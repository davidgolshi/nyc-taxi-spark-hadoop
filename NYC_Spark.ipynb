{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES\n",
    "# TODO: Join Geo Taxi data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import dotenv\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import \\\n",
    "    StructType, StructField, StringType, LongType, DoubleType, TimestampType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from hdfs import InsecureClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environemnt Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "SPARK_CUDF_JAR = os.getenv('SPARK_CUDF_JAR')\n",
    "SPARK_RAPIDS_PLUGIN_JAR = os.getenv('SPARK_RAPIDS_PLUGIN_JAR')\n",
    "SPARK_GPU_DISCOVERY_SCRIPT = os.getenv('SPARK_GPU_DISCOVERY_SCRIPT')\n",
    "namenode_URI = os.getenv('namenode_URI')\n",
    "hadoop_user = os.getenv('hadoop_user')\n",
    "hdfs_path = os.getenv('hdfs_path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Spark, Connection to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: sparl.executor.resource.gpu.amount\n",
      "Warning: Ignoring non-Spark config property: sparl.task.resource.gpu.amount\n",
      "2021-10-28 17:52:05,813 WARN util.Utils: Your hostname, david-Z97X-Gaming-7 resolves to a loopback address: 127.0.1.1; using 192.168.0.159 instead (on interface enp42s0)\n",
      "2021-10-28 17:52:05,813 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/hadoop/miniconda3/envs/pyspark/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "2021-10-28 17:52:05,991 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2021-10-28 17:52:10,640 WARN rapids.SQLExecPlugin: RAPIDS Accelerator 21.08.0 using cudf 21.08.2. To disable GPU support set `spark.rapids.sql.enabled` to false\n",
      "2021-10-28 17:52:10,641 WARN udf.Plugin: Installing rapids UDF compiler extensions to Spark. The compiler is disabled by default. To enable it, set `spark.rapids.sql.udfCompiler.enabled` to true\n"
     ]
    }
   ],
   "source": [
    "# configure environment & instantiate sparksession\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('nyc_taxi') \\\n",
    "    .config('spark.plugins', 'com.nvidia.spark.SQLPlugin') \\\n",
    "    .config('spark.jars', f'{SPARK_CUDF_JAR},{SPARK_RAPIDS_PLUGIN_JAR}') \\\n",
    "    .config('spark.rapids.sql.enabled', 'true') \\\n",
    "    .config('spark.rapids.sql.incompatibleOps.enabled', 'true') \\\n",
    "    .config('sparl.executor.resource.gpu.amount', '1') \\\n",
    "    .config('sparl.task.resource.gpu.amount', '0.25') \\\n",
    "    .config('spark.driver.memory', '20g') \\\n",
    "    .config('spark.task.cpus', '4') \\\n",
    "    .config('spark.sql.shuffle.partitions', '20') \\\n",
    "    .config('spark.driver.maxResultSize', '10g') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://192.168.0.159:4040'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url for Spark UI for monitoring\n",
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yellow_taxi_2017-01.parquet', 'yellow_taxi_2017-02.parquet', 'yellow_taxi_2017-03.parquet', 'yellow_taxi_2017-04.parquet', 'yellow_taxi_2017-05.parquet', 'yellow_taxi_2017-06.parquet', 'yellow_taxi_2017-07.parquet', 'yellow_taxi_2017-08.parquet', 'yellow_taxi_2017-09.parquet', 'yellow_taxi_2017-10.parquet', 'yellow_taxi_2017-11.parquet', 'yellow_taxi_2017-12.parquet', 'yellow_taxi_2018-01.parquet', 'yellow_taxi_2018-02.parquet', 'yellow_taxi_2018-03.parquet', 'yellow_taxi_2018-04.parquet', 'yellow_taxi_2018-05.parquet', 'yellow_taxi_2018-06.parquet', 'yellow_taxi_2018-07.parquet', 'yellow_taxi_2018-08.parquet', 'yellow_taxi_2018-09.parquet', 'yellow_taxi_2018-10.parquet', 'yellow_taxi_2018-11.parquet', 'yellow_taxi_2018-12.parquet', 'yellow_taxi_2019-01.parquet', 'yellow_taxi_2019-02.parquet', 'yellow_taxi_2019-03.parquet', 'yellow_taxi_2019-04.parquet', 'yellow_taxi_2019-05.parquet', 'yellow_taxi_2019-06.parquet', 'yellow_taxi_2019-07.parquet', 'yellow_taxi_2019-08.parquet', 'yellow_taxi_2019-09.parquet', 'yellow_taxi_2019-10.parquet', 'yellow_taxi_2019-11.parquet', 'yellow_taxi_2019-12.parquet', 'yellow_taxi_2020-01.parquet', 'yellow_taxi_2020-02.parquet', 'yellow_taxi_2020-03.parquet', 'yellow_taxi_2020-04.parquet', 'yellow_taxi_2020-05.parquet', 'yellow_taxi_2020-06.parquet', 'yellow_taxi_2020-07.parquet', 'yellow_taxi_2020-08.parquet', 'yellow_taxi_2020-09.parquet', 'yellow_taxi_2020-10.parquet', 'yellow_taxi_2020-11.parquet', 'yellow_taxi_2020-12.parquet']\n",
      "number of files in dfs directory: 48\n"
     ]
    }
   ],
   "source": [
    "# list files in hdfs directory, count files in directory\n",
    "hdfs = InsecureClient(namenode_URI, user=hadoop_user)\n",
    "fs = hdfs.list(hdfs_path)\n",
    "print(fs)\n",
    "print('number of files in dfs directory:', len(fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'VendorID': 48, 'tpep_pickup_datetime': 48, 'tpep_dropoff_datetime': 48, 'passenger_count': 48, 'trip_distance': 48, 'RatecodeID': 48, 'store_and_fwd_flag': 48, 'PULocationID': 48, 'DOLocationID': 48, 'payment_type': 48, 'fare_amount': 48, 'extra': 48, 'mta_tax': 48, 'tip_amount': 48, 'tolls_amount': 48, 'improvement_surcharge': 48, 'total_amount': 48, 'congestion_surcharge': 24})\n"
     ]
    }
   ],
   "source": [
    "# Get all column names in data\n",
    "columns = []\n",
    "for prqt in hdfs.list(hdfs_path):\n",
    "    path = os.path.join('hdfs://localhost:9000/', hdfs_path, prqt)\n",
    "    cols = spark.read \\\n",
    "        .options(header='true', inferschema='true') \\\n",
    "        .parquet(path) \\\n",
    "        .limit(1) \\\n",
    "        .columns\n",
    "    \n",
    "    columns.extend(cols)\n",
    "\n",
    "# see what columns are similar between\n",
    "counter = Counter(columns)\n",
    "print(counter)\n",
    "\n",
    "# lets take all columns that are present within all datasets (48 maximum columns count)\n",
    "columns = [i[0] for i in counter.items() if i[1] == max([i[1]for i in counter.items()])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the schema for data\n",
    "customSchema = StructType([\n",
    "    StructField('VendorID', LongType(), True),\n",
    "    StructField('tpep_pickup_datetime', TimestampType(), True),\n",
    "    StructField('tpep_dropoff_datetime', TimestampType(), True),\n",
    "    StructField('passenger_count', LongType(), True),\n",
    "    StructField('trip_distance', DoubleType(), True),\n",
    "    StructField('RatecodeID', LongType(), True),\n",
    "    StructField('store_and_fwd_flag', StringType(), True),\n",
    "    StructField('PULocationID', LongType(), True),\n",
    "    StructField('DOLocationID', LongType(), True),\n",
    "    StructField('payment_type', LongType(), True),\n",
    "    StructField('fare_amount', DoubleType(), True),\n",
    "    StructField('extra', DoubleType(), True),\n",
    "    StructField('mta_tax', DoubleType(), True),\n",
    "    StructField('tip_amount', DoubleType(), True),\n",
    "    StructField('tolls_amount', DoubleType(), True),\n",
    "    StructField('improvement_surcharge', DoubleType(), True),\n",
    "    StructField('total_amount', DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "path = os.path.join('hdfs://localhost:9000/', hdfs_path, '*')\n",
    "df = spark.read \\\n",
    "    .format('parquet') \\\n",
    "    .schema(customSchema) \\\n",
    "    .load(path) \\\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows:  325.35 Million \n",
      "number of columns:  17 \n",
      "total count of data:  5.53 Billion\n"
     ]
    }
   ],
   "source": [
    "# preview of concatenated dataframe size\n",
    "print('number of rows: ', round(df.count() / 1e6, 2), 'Million',\n",
    "      '\\nnumber of columns: ', len(df.columns),\n",
    "      '\\ntotal count of data: ', round(df.count() * len(df.columns) / 1e9, 2), 'Billion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load location lookup table\n",
    "lfp = './data/taxi_location_lookup_table.csv'\n",
    "lookup_table = pd.read_csv(lfp)\n",
    "\n",
    "columns = lookup_table.columns\n",
    "schema = StructType([\n",
    "    StructField(columns[0], IntegerType(), True),\n",
    "    StructField(columns[1], StringType(), True),\n",
    "    StructField(columns[2], StringType(), True),\n",
    "    StructField(columns[3], StringType(), True),\n",
    "])\n",
    "lookup_table = spark.createDataFrame(lookup_table, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load geographic data (shapefile)\n",
    "gfp = './data/geo_files/taxi_zones.shp'\n",
    "gdf = gpd.read_file(gfp)\n",
    "gdf = gdf[['LocationID', 'Shape_Leng', 'Shape_Area', 'zone', 'geometry']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VendorID</th>\n",
       "      <td>1056169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger_count</th>\n",
       "      <td>1056169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_distance</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RatecodeID</th>\n",
       "      <td>1056169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PULocationID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOLocationID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payment_type</th>\n",
       "      <td>1056169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fare_amount</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extra</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mta_tax</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tip_amount</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tolls_amount</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_amount</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count_na\n",
       "VendorID                1056169\n",
       "tpep_pickup_datetime          0\n",
       "tpep_dropoff_datetime         0\n",
       "passenger_count         1056169\n",
       "trip_distance                 0\n",
       "RatecodeID              1056169\n",
       "store_and_fwd_flag            0\n",
       "PULocationID                  0\n",
       "DOLocationID                  0\n",
       "payment_type            1056169\n",
       "fare_amount                   0\n",
       "extra                         0\n",
       "mta_tax                       0\n",
       "tip_amount                    0\n",
       "tolls_amount                  0\n",
       "improvement_surcharge         0\n",
       "total_amount                  0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for null values in dataframe\n",
    "df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]) \\\n",
    "    .toPandas() \\\n",
    "    .transpose() \\\n",
    "    .rename(columns={0: 'count_na'})\n",
    "\n",
    "# seems like there are issues with about 1M rows of the data missing VendorID, passenger_count, RatecodeID, payment_type\n",
    "# remaining columns have no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dates with null values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year(tpep_pickup_datetime)</th>\n",
       "      <th>month(tpep_pickup_datetime)</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>33970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>33235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>34197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>46826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>47311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>51198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>65347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>48760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>37474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>19671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>59259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020</td>\n",
       "      <td>6</td>\n",
       "      <td>50773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020</td>\n",
       "      <td>7</td>\n",
       "      <td>62871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020</td>\n",
       "      <td>8</td>\n",
       "      <td>70753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>90248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020</td>\n",
       "      <td>10</td>\n",
       "      <td>105846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020</td>\n",
       "      <td>11</td>\n",
       "      <td>99267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020</td>\n",
       "      <td>12</td>\n",
       "      <td>99002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year(tpep_pickup_datetime)  month(tpep_pickup_datetime)   count\n",
       "3                         2019                            6     161\n",
       "16                        2019                            7   33970\n",
       "12                        2019                            8   33235\n",
       "13                        2019                            9   34197\n",
       "8                         2019                           10   46826\n",
       "1                         2019                           11   47311\n",
       "7                         2019                           12   51198\n",
       "0                         2020                            1   65347\n",
       "15                        2020                            2   48760\n",
       "9                         2020                            3   37474\n",
       "18                        2020                            4   19671\n",
       "4                         2020                            5   59259\n",
       "14                        2020                            6   50773\n",
       "10                        2020                            7   62871\n",
       "11                        2020                            8   70753\n",
       "2                         2020                            9   90248\n",
       "17                        2020                           10  105846\n",
       "6                         2020                           11   99267\n",
       "5                         2020                           12   99002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dates of null value rows\n",
    "print('dates with null values')\n",
    "df.filter(F.col('VendorID').isNull()) \\\n",
    "    .groupBy(F.year('tpep_pickup_datetime'), F.month('tpep_pickup_datetime')) \\\n",
    "    .count().alias('count_na') \\\n",
    "    .toPandas().sort_values(by=['year(tpep_pickup_datetime)', 'month(tpep_pickup_datetime)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with null values\n",
    "df_drop = df.filter(~F.col('VendorID').isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique years: [Row(year(tpep_pickup_datetime)=2009), Row(year(tpep_pickup_datetime)=2019), Row(year(tpep_pickup_datetime)=2058), Row(year(tpep_pickup_datetime)=2090), Row(year(tpep_pickup_datetime)=2008), Row(year(tpep_pickup_datetime)=2001), Row(year(tpep_pickup_datetime)=2017), Row(year(tpep_pickup_datetime)=2016), Row(year(tpep_pickup_datetime)=2084), Row(year(tpep_pickup_datetime)=2031), Row(year(tpep_pickup_datetime)=2053), Row(year(tpep_pickup_datetime)=2002), Row(year(tpep_pickup_datetime)=2088), Row(year(tpep_pickup_datetime)=2066), Row(year(tpep_pickup_datetime)=2003), Row(year(tpep_pickup_datetime)=2042), Row(year(tpep_pickup_datetime)=2029), Row(year(tpep_pickup_datetime)=2033), Row(year(tpep_pickup_datetime)=2041), Row(year(tpep_pickup_datetime)=2018), Row(year(tpep_pickup_datetime)=2026), Row(year(tpep_pickup_datetime)=2038), Row(year(tpep_pickup_datetime)=2037), Row(year(tpep_pickup_datetime)=2021), Row(year(tpep_pickup_datetime)=2000), Row(year(tpep_pickup_datetime)=2015), Row(year(tpep_pickup_datetime)=2020), Row(year(tpep_pickup_datetime)=2010), Row(year(tpep_pickup_datetime)=2032)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows with wrong years: 113510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# we should only have data on years 2017 - 2020\n",
    "print('unique years:', df.select(F.year(df.tpep_pickup_datetime)).distinct().collect())\n",
    "\n",
    "# check how many rows of data are not within 2017-2020\n",
    "years = ['2017', '2018', '2019', '2020']\n",
    "print('rows with wrong years:', df.filter(~F.year(df.tpep_pickup_datetime).isin(years)).count())\n",
    "\n",
    "# 113.5k relatively small amount of taxi rides compared to 325.3m, we can drop these rows\n",
    "df = df.filter(F.year(df.tpep_pickup_datetime).isin(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "[Row(month(tpep_pickup_datetime)=10),\n",
       " Row(month(tpep_pickup_datetime)=4),\n",
       " Row(month(tpep_pickup_datetime)=1),\n",
       " Row(month(tpep_pickup_datetime)=8),\n",
       " Row(month(tpep_pickup_datetime)=11),\n",
       " Row(month(tpep_pickup_datetime)=12),\n",
       " Row(month(tpep_pickup_datetime)=5),\n",
       " Row(month(tpep_pickup_datetime)=7),\n",
       " Row(month(tpep_pickup_datetime)=9),\n",
       " Row(month(tpep_pickup_datetime)=6),\n",
       " Row(month(tpep_pickup_datetime)=3),\n",
       " Row(month(tpep_pickup_datetime)=2)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the months within the data\n",
    "df.select(F.month(df.tpep_pickup_datetime)).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique years: [Row(year(tpep_dropoff_datetime)=1926), Row(year(tpep_dropoff_datetime)=2019), Row(year(tpep_dropoff_datetime)=2016), Row(year(tpep_dropoff_datetime)=2017), Row(year(tpep_dropoff_datetime)=1998), Row(year(tpep_dropoff_datetime)=1997), Row(year(tpep_dropoff_datetime)=2018), Row(year(tpep_dropoff_datetime)=2021), Row(year(tpep_dropoff_datetime)=2020)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows with wrong years: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# check if the years are correct like the pickup datetime\n",
    "print('unique years:', df.select(F.year(df.tpep_dropoff_datetime)).distinct().collect())\n",
    "\n",
    "# there are extra rows with improper years, check the amount of bad data\n",
    "print('rows with wrong years:', df.filter(~F.year(df.tpep_dropoff_datetime).isin(years)).count())\n",
    "\n",
    "# only 75 rows, we can drop them\n",
    "df = df.filter(F.year(df.tpep_dropoff_datetime).isin(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "[Row(month(tpep_dropoff_datetime)=4),\n",
       " Row(month(tpep_dropoff_datetime)=10),\n",
       " Row(month(tpep_dropoff_datetime)=1),\n",
       " Row(month(tpep_dropoff_datetime)=11),\n",
       " Row(month(tpep_dropoff_datetime)=8),\n",
       " Row(month(tpep_dropoff_datetime)=12),\n",
       " Row(month(tpep_dropoff_datetime)=5),\n",
       " Row(month(tpep_dropoff_datetime)=7),\n",
       " Row(month(tpep_dropoff_datetime)=6),\n",
       " Row(month(tpep_dropoff_datetime)=9),\n",
       " Row(month(tpep_dropoff_datetime)=3),\n",
       " Row(month(tpep_dropoff_datetime)=2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the months within the dates\n",
    "df.select(F.month(df.tpep_dropoff_datetime)).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attributes of dates\n",
    "df = df.withColumn('pickup_year', F.year('tpep_pickup_datetime')) \\\n",
    "       .withColumn('pickup_month', F.month('tpep_pickup_datetime')) \\\n",
    "       .withColumn('pickup_week', F.weekofyear('tpep_pickup_datetime')) \\\n",
    "       .withColumn('ride_duration', (F.unix_timestamp('tpep_dropoff_datetime') - F.unix_timestamp('tpep_pickup_datetime')) / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# get total fares and count of rides by pick up drop off and time location pairs\n",
    "df = df.groupBy('PULocationID', 'DOLocationID', 'pickup_year', 'pickup_month', 'pickup_week') \\\n",
    "                        .agg(F.sum('fare_amount').alias('total_fare'),\n",
    "                             F.count('fare_amount').alias('num_rides'),\n",
    "                             F.mean('ride_duration').alias('avg_duration')) \\\n",
    "                         .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Geographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>zone</th>\n",
       "      <th>geometry</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>pickup_year</th>\n",
       "      <th>pickup_month</th>\n",
       "      <th>pickup_week</th>\n",
       "      <th>total_fare</th>\n",
       "      <th>num_rides</th>\n",
       "      <th>avg_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>POLYGON ((933100.918 192536.086, 933091.011 19...</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>POLYGON ((933100.918 192536.086, 933091.011 19...</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>113.5</td>\n",
       "      <td>1</td>\n",
       "      <td>87.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>POLYGON ((933100.918 192536.086, 933091.011 19...</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1</td>\n",
       "      <td>53.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>POLYGON ((933100.918 192536.086, 933091.011 19...</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>67.5</td>\n",
       "      <td>1</td>\n",
       "      <td>34.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>POLYGON ((933100.918 192536.086, 933091.011 19...</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1</td>\n",
       "      <td>88.033333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID  Shape_Leng  Shape_Area            zone  \\\n",
       "0           1    0.116357    0.000782  Newark Airport   \n",
       "1           1    0.116357    0.000782  Newark Airport   \n",
       "2           1    0.116357    0.000782  Newark Airport   \n",
       "3           1    0.116357    0.000782  Newark Airport   \n",
       "4           1    0.116357    0.000782  Newark Airport   \n",
       "\n",
       "                                            geometry  PULocationID  \\\n",
       "0  POLYGON ((933100.918 192536.086, 933091.011 19...             1   \n",
       "1  POLYGON ((933100.918 192536.086, 933091.011 19...             1   \n",
       "2  POLYGON ((933100.918 192536.086, 933091.011 19...             1   \n",
       "3  POLYGON ((933100.918 192536.086, 933091.011 19...             1   \n",
       "4  POLYGON ((933100.918 192536.086, 933091.011 19...             1   \n",
       "\n",
       "   DOLocationID  pickup_year  pickup_month  pickup_week  total_fare  \\\n",
       "0           265         2017             2            5         4.5   \n",
       "1           132         2017             3            9       113.5   \n",
       "2           132         2017             3           10        92.0   \n",
       "3           161         2017             5           18        67.5   \n",
       "4            50         2017             6           22       106.0   \n",
       "\n",
       "   num_rides  avg_duration  \n",
       "0          1      1.366667  \n",
       "1          1     87.550000  \n",
       "2          1     53.833333  \n",
       "3          1     34.066667  \n",
       "4          1     88.033333  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join Pickup and Drop off geodata\n",
    "taxi_df = df.merge() \n",
    "\n",
    "gdf = gdf.merge(df, left_on='LocationID', right_on='PULocationID').drop('LocationID')\n",
    "gdf = gdf.rename({'Shape_Leng':'pu_Shape_Leng',\n",
    "                    'Shape_Area':'pu_Shape_Area',\n",
    "                    'zone':'pu_zone',\n",
    "                    'geometry':'pu_geometry'})\n",
    "\n",
    "gdf = gdf.merge(df, left_on='LocationID', right_on='PULocationID')\n",
    "gdf = gdf.rename({'Shape_Leng':'do_Shape_Leng',\n",
    "                    'Shape_Area':'do_Shape_Area',\n",
    "                    'zone':'do_zone',\n",
    "                    'geometry':'do_geometry'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# RDD implementation of fares_by_locations (GPU not utilized)\n",
    "fares_by_locations = df.select(['PULocationID', 'DOLocationID', 'fare_amount']).rdd \\\n",
    "                         .map(lambda x: ((x[0], x[1]), x[2])) \\\n",
    "                         .reduceByKey(lambda x, y: x + y) \\\n",
    "                         .toDF().toPandas()\n",
    "time: 10m 53s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# user defined function for finding difference between two timestamps in minutes\n",
    "# best practices recommend staying away from using UDFs\n",
    "\n",
    "time_difference_udf = F.udf(lambda pickup, dropoff: (F.unix_timestamp(dropoff) - F.unix_timestamp(pickup)) / 60, DoubleType())\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76dfc580577791d80f3ac4222c0ceb470623d59777f0b3fcc8fbba9b85efd2c6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pyspark': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
